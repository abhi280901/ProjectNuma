{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adf668a-b68d-410e-a5a7-2a3df6994bc6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54964b5d-c5ad-4a78-9a55-95d3df31f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f994085-6e5e-46a5-8a0b-15942d394b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55de1e95-8f37-482d-98ba-d2870de16506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2138275 entries, 0 to 2138274\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   pca_0                   float64\n",
      " 1   pca_1                   float64\n",
      " 2   pca_2                   float64\n",
      " 3   pca_3                   float64\n",
      " 4   pca_4                   float64\n",
      " 5   laundering_schema_type  int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 97.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_parquet('../BIS_data/Final_BIS_Data.parquet')\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031b6f1-15b1-4d22-ad45-72996ef14897",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b792d7-696a-4471-a6c8-71068d12bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Split Data into Train and Test Sets BEFORE Undersampling\n",
    "# ---------------------------\n",
    "X = df_full.drop('laundering_schema_type', axis=1)\n",
    "y = df_full['laundering_schema_type']\n",
    "\n",
    "# Use stratify=y to preserve the class distribution in the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c16e92-da8f-4f6e-b71c-88e19bfcb58e",
   "metadata": {},
   "source": [
    "# 3. Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb22a67-9a55-4438-977e-6d2144bc026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\BIS_env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\envs\\BIS_env\\lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\anaconda3\\envs\\BIS_env\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Undersample Only the Training Set\n",
    "# ---------------------------\n",
    "# Combine training features and labels for processing.\n",
    "train_df = X_train.copy()\n",
    "train_df['laundering_schema_type'] = y_train\n",
    "\n",
    "# Separate minority (fraud) and majority (non-fraud) classes in the training data.\n",
    "fraud_train = train_df[train_df['laundering_schema_type'] == 1]\n",
    "\n",
    "# Undersample the majority class to match the number of fraud samples.\n",
    "n_samples = len(fraud_train)\n",
    "\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n_samples}, random_state=42)\n",
    "X_under, y_under = rus.fit_resample(X_train.values, y_train.values)\n",
    "\n",
    "train_df_undersampled = pd.DataFrame(X_under, columns=[f'pca_{i}' for i in range(X_under.shape[1])])\n",
    "train_df_undersampled['laundering_schema_type'] = y_under\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e1a1b4-5fcc-403f-b8a4-47becaa46dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for the undersampled training data.\n",
    "X_train = train_df_undersampled.drop('laundering_schema_type', axis=1).values\n",
    "y_train = train_df_undersampled['laundering_schema_type'].values\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "# For the test set, keep the original (imbalanced) distribution.\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Convert the datasets to PyTorch tensors.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097e86-3a59-4713-ae5e-9e13963d2250",
   "metadata": {},
   "source": [
    "# 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e4cf73-a414-472f-ba06-40bc688deb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Define the MLP Model\n",
    "# ---------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class FraudResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=3):\n",
    "        super(FraudResNet, self).__init__()\n",
    "        # Initial projection layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_dim) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c920a6f-e5e7-4930-8333-433cb3c1b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 2  # Two neurons for two classes\n",
    "num_blocks = 3  # Number of residual blocks\n",
    "\n",
    "model = FraudResNet(input_dim, hidden_dim, output_dim, num_blocks).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fde5e0-15e9-4daf-a833-11687be13a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Setup Loss, Optimizer, and Training Parameters\n",
    "# ---------------------------\n",
    "# Since the training data is now balanced, we can use the default weights.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "best_loss = float('inf')\n",
    "best_model_path = \"./Model_Weight/ResNet_best_undersampling_BIS.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8169fa1-5ec4-46df-ac4e-a5b9c47d7cfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200], Loss: 0.7774\n",
      "Epoch [10/200], Loss: 0.5921\n",
      "Epoch [15/200], Loss: 0.4929\n",
      "Epoch [20/200], Loss: 0.4389\n",
      "Epoch [25/200], Loss: 0.4066\n",
      "Epoch [30/200], Loss: 0.3866\n",
      "Epoch [35/200], Loss: 0.3718\n",
      "Epoch [40/200], Loss: 0.3604\n",
      "Epoch [45/200], Loss: 0.3516\n",
      "Epoch [50/200], Loss: 0.3446\n",
      "Epoch [55/200], Loss: 0.3390\n",
      "Epoch [60/200], Loss: 0.3343\n",
      "Epoch [65/200], Loss: 0.3303\n",
      "Epoch [70/200], Loss: 0.3268\n",
      "Epoch [75/200], Loss: 0.3238\n",
      "Epoch [80/200], Loss: 0.3210\n",
      "Epoch [85/200], Loss: 0.3185\n",
      "Epoch [90/200], Loss: 0.3162\n",
      "Epoch [95/200], Loss: 0.3141\n",
      "Epoch [100/200], Loss: 0.3122\n",
      "Epoch [105/200], Loss: 0.3104\n",
      "Epoch [110/200], Loss: 0.3088\n",
      "Epoch [115/200], Loss: 0.3072\n",
      "Epoch [120/200], Loss: 0.3058\n",
      "Epoch [125/200], Loss: 0.3045\n",
      "Epoch [130/200], Loss: 0.3033\n",
      "Epoch [135/200], Loss: 0.3021\n",
      "Epoch [140/200], Loss: 0.3010\n",
      "Epoch [145/200], Loss: 0.3000\n",
      "Epoch [150/200], Loss: 0.2990\n",
      "Epoch [155/200], Loss: 0.2981\n",
      "Epoch [160/200], Loss: 0.2972\n",
      "Epoch [165/200], Loss: 0.2964\n",
      "Epoch [170/200], Loss: 0.2957\n",
      "Epoch [175/200], Loss: 0.2950\n",
      "Epoch [180/200], Loss: 0.2943\n",
      "Epoch [185/200], Loss: 0.2936\n",
      "Epoch [190/200], Loss: 0.2930\n",
      "Epoch [195/200], Loss: 0.2924\n",
      "Epoch [200/200], Loss: 0.2918\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6. Training Loop with Checkpointing (Saving Best Model by Training Loss)\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save the model if training loss improved.\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938285cc-a4bd-4653-a7dd-5d235895b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92    200000\n",
      "           1       0.31      0.87      0.45     13828\n",
      "\n",
      "    accuracy                           0.86    213828\n",
      "   macro avg       0.65      0.87      0.69    213828\n",
      "weighted avg       0.95      0.86      0.89    213828\n",
      "\n",
      "[[172688  27312]\n",
      " [  1768  12060]]\n",
      "F1-score: 0.45338345864661656\n",
      "Precision-score: 0.30630905211825665\n",
      "Recall-score: 0.872143477003182\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluation on the Test Set (with the Original Imbalanced Distribution)\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on (Test Data):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95474316-e4c2-4438-9254-ae87abc3ad6b",
   "metadata": {},
   "source": [
    "# 5. Evaluate on Unseen Data (Without Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203968e-4006-4d59-bfdf-1105631c49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c1748-56fa-4eb0-9d3d-a8ad190c74b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Without Fine-tuning\n",
    "# ---------------------------\n",
    "# 8. Load the Best Model into a New Instance and Test on Unseen Data (e.g., SetB)\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "best_model_path = \"./Model_Final/CNN_best_model_undersampling_final_BIS_V2.pt\"\n",
    "\n",
    "loaded_model = FraudResNet(5, 64, 2, 3).to(device)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9593ed-ab20-437c-bdd4-7908fd446ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_files_unseen_1 = glob.glob(\"./BIS_data/client_part7_1.npy\")\n",
    "\n",
    "# Load each npy file into a list\n",
    "client_data_list_unseen_1 = [np.load(file, allow_pickle=True) for file in npy_files_unseen_1]\n",
    "\n",
    "# Combine all client data into one numpy array\n",
    "combined_data_unseen_1 = np.vstack(client_data_list_unseen_1)\n",
    "\n",
    "col_names = [f'pca_{i}' for i in range(5)] + ['laundering_schema_type', 'laundering_schema_id']\n",
    "\n",
    "# Convert the combined array into a DataFrame\n",
    "new_df = pd.DataFrame(combined_data_unseen_1, columns=col_names)\n",
    "new_df = new_df.drop('laundering_schema_id', axis=1)\n",
    "new_df = new_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "new_df['laundering_schema_type'] = new_df['laundering_schema_type'].notna().astype(int)\n",
    "new_df = new_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfda276-2d2b-423b-876d-0f1d1e982916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume you have an unseen dataset (SetB) with the same attributes.\n",
    "# new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "\n",
    "if 'laundering_schema_type' in new_df.columns:\n",
    "    X_new = new_df.drop('laundering_schema_type', axis=1).values\n",
    "    y_new = new_df['laundering_schema_type'].values\n",
    "else:\n",
    "    X_new = new_df.values\n",
    "    y_new = None\n",
    "\n",
    "X_new = X_new.astype(np.float32)\n",
    "X_new = torch.tensor(X_new, dtype=torch.float).to(device)\n",
    "if y_new is not None:\n",
    "    y_new = torch.tensor(y_new, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_outputs = loaded_model(X_new)\n",
    "    _, new_predicted = torch.max(new_outputs, 1)\n",
    "    new_predictions = new_predicted.cpu().numpy()\n",
    "\n",
    "if y_new is not None:\n",
    "    # Convert y_new tensor to numpy array for metric calculations.\n",
    "    y_new_np = y_new.cpu().numpy()\n",
    "    print(\"\\nEvaluation on Unseen Data (Set7_part1):\")\n",
    "    print(classification_report(y_new_np, new_predictions))\n",
    "    print(confusion_matrix(y_new_np, new_predictions))\n",
    "    print(\"F1-score:\", f1_score(y_new_np, new_predictions))\n",
    "    print(\"Precision-score:\", precision_score(y_new_np, new_predictions))\n",
    "    print(\"Recall-score:\", recall_score(y_new_np, new_predictions))\n",
    "else:\n",
    "    print(\"\\nPredictions on Unseen Data (Set7_part1):\")\n",
    "    print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d720a-64a1-40ae-9326-7330f70b76e6",
   "metadata": {},
   "source": [
    "# 6. Evaluate on Unseen Data (With Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad42249-4baa-45d7-9747-6fa26ed161e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "# best_model_path = \"./Model_Final/best_model_undersampling_final_all.pt\"\n",
    "\n",
    "# # Separate features and labels.\n",
    "# X_new = new_df.drop('Class', axis=1).values\n",
    "# y_new = new_df['Class'].values\n",
    "\n",
    "# # Split the unseen data into a fine-tuning training set and a test set.\n",
    "# X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(\n",
    "#     X_new, y_new, test_size=0.5, random_state=42, stratify=y_new\n",
    "# )\n",
    "\n",
    "# # Convert to PyTorch tensors.\n",
    "# X_new_train = torch.tensor(X_new_train, dtype=torch.float)\n",
    "# y_new_train = torch.tensor(y_new_train, dtype=torch.long)\n",
    "# X_new_test  = torch.tensor(X_new_test, dtype=torch.float)\n",
    "# y_new_test  = torch.tensor(y_new_test, dtype=torch.long)\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2. Load the Pre-Trained Model from SetA\n",
    "# # ---------------------------\n",
    "# # Create a new model instance with the same architecture.\n",
    "# class FraudMLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(FraudMLP, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(hidden_dim),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# input_dim = X_new_train.shape[1]\n",
    "# hidden_dim = 64\n",
    "# output_dim = 2  # Two neurons for two classes\n",
    "\n",
    "# model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# loaded_model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "# loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# # It's often useful to set the model in train mode during fine-tuning.\n",
    "# loaded_model.train()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3. Fine-Tuning on Unseen Data (SetB)\n",
    "# # ---------------------------\n",
    "# # Use a lower learning rate for fine-tuning.\n",
    "# finetune_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-3)\n",
    "# finetune_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# finetune_epochs = 300  # Adjust as needed\n",
    "\n",
    "# print(\"\\n--- Fine-Tuning on Unseen Data (SetB) ---\")\n",
    "# for epoch in range(finetune_epochs):\n",
    "#     finetune_optimizer.zero_grad()\n",
    "#     outputs = loaded_model(X_new_train)\n",
    "#     loss = finetune_criterion(outputs, y_new_train)\n",
    "#     loss.backward()\n",
    "#     finetune_optimizer.step()\n",
    "    \n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         print(f\"Fine-Tuning Epoch [{epoch + 1}/{finetune_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969268f-661b-48a7-8684-820ccc9aff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # 4. Evaluate the Fine-Tuned Model on SetB Test Data\n",
    "# # ---------------------------\n",
    "# loaded_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_outputs = loaded_model(X_new_test)\n",
    "#     _, predicted = torch.max(test_outputs, 1)\n",
    "#     y_pred = predicted.numpy()\n",
    "#     y_true = y_new_test.numpy()\n",
    "    \n",
    "#     print(\"\\nEvaluation on Fine-Tuned Unseen Data (SetB Test):\")\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(confusion_matrix(y_true, y_pred))\n",
    "#     print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "#     print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "#     print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5f4e2-3624-4167-9310-d3022cc554a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
