{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adf668a-b68d-410e-a5a7-2a3df6994bc6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54964b5d-c5ad-4a78-9a55-95d3df31f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a42f57-d95c-4aff-823b-7320afcf2a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d891b909-e15c-4ae0-be91-ca4d72ebdb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2138275 entries, 0 to 2138274\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   pca_0                   float64\n",
      " 1   pca_1                   float64\n",
      " 2   pca_2                   float64\n",
      " 3   pca_3                   float64\n",
      " 4   pca_4                   float64\n",
      " 5   laundering_schema_type  int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 97.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_parquet('../BIS_data/Final_BIS_Data.parquet')\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031b6f1-15b1-4d22-ad45-72996ef14897",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b792d7-696a-4471-a6c8-71068d12bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Split Data into Train and Test Sets BEFORE Undersampling\n",
    "# ---------------------------\n",
    "X = df_full.drop('laundering_schema_type', axis=1)\n",
    "y = df_full['laundering_schema_type']\n",
    "\n",
    "# Use stratify=y to preserve the class distribution in the test set.\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c16e92-da8f-4f6e-b71c-88e19bfcb58e",
   "metadata": {},
   "source": [
    "# 3. SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee82b0-f75a-4dc3-b531-285ac87e3a06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.1 Undersampling + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1421da8-6caf-4a25-9590-3aa6e400989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling: Counter({np.int64(0): 1000000, np.int64(1): 110620})\n",
      "After oversampling: Counter({np.int64(0): 1000000, np.int64(1): 1000000})\n",
      "\n",
      "Resampled dataset shape: (2000000, 6)\n",
      "Resampled class distribution:\n",
      "laundering_schema_type\n",
      "0    1000000\n",
      "1    1000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Define thresholds for undersampling and oversampling.\n",
    "# For production, set these to 1_000_000. Here we use 500 for a demonstration.\n",
    "# =============================================================================\n",
    "majority_class = 0\n",
    "minority_class = 1\n",
    "\n",
    "undersample_target = 1000000  # Replace with 1_000_000 in production\n",
    "oversample_target  = 1000000  # Replace with 1_000_000 in production\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Separate features and target.\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Undersample the majority class.\n",
    "# This step reduces the majority class to the specified threshold.\n",
    "# =============================================================================\n",
    "rus = RandomUnderSampler(sampling_strategy={majority_class: undersample_target}, random_state=42)\n",
    "\n",
    "X_under, y_under = rus.fit_resample(X_train_ori.values, y_train_ori.values)\n",
    "\n",
    "print(\"After undersampling:\", Counter(y_under))\n",
    "# At this point, the majority class count is now `undersample_target` while the minority remains unchanged.\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Oversample the minority class using SMOTE.\n",
    "# This step increases the minority class to the specified threshold.\n",
    "# =============================================================================\n",
    "# SMOTE requires at least (k_neighbors + 1) samples in the minority class. Adjust if needed.\n",
    "current_minority_count = Counter(y_under)[minority_class]\n",
    "\n",
    "k_neighbors = 5 if current_minority_count > 5 else current_minority_count - 1\n",
    "\n",
    "smote = SMOTE(sampling_strategy={minority_class: oversample_target}, k_neighbors=k_neighbors, random_state=42)\n",
    "\n",
    "X_res, y_res = smote.fit_resample(X_under, y_under)\n",
    "print(\"After oversampling:\", Counter(y_res))\n",
    "# Now, both classes should have the target number of samples.\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Combine the resampled data into a DataFrame.\n",
    "# =============================================================================\n",
    "df_resampled = pd.DataFrame(X_res, columns=[f'pca_{i}' for i in range(X_res.shape[1])])\n",
    "df_resampled['laundering_schema_type'] = y_res\n",
    "\n",
    "print(\"\\nResampled dataset shape:\", df_resampled.shape)\n",
    "print(\"Resampled class distribution:\")\n",
    "print(df_resampled['laundering_schema_type'].value_counts())\n",
    "df_resampled = df_resampled.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bb8796-ee81-4d73-affb-0c5d5c8e18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for the undersampled training data.\n",
    "X_train = df_resampled.drop('laundering_schema_type', axis=1).values\n",
    "y_train = df_resampled['laundering_schema_type'].values\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "# For the test set, keep the original (imbalanced) distribution.\n",
    "X_test = X_test_ori.values\n",
    "y_test = y_test_ori.values\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Convert the datasets to PyTorch tensors.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e0cfb-d5cd-4e0a-b31a-7a16e94c12c5",
   "metadata": {},
   "source": [
    "### 3.2 Default SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06597abe-3026-4dd8-99e6-fdbc9a50f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. SMOTE Only the Training Set\n",
    "# ---------------------------\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_ori, y_train_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee8bb68-783b-42b6-be08-f92c0bf9e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for the undersampled training data.\n",
    "X_train = X_train_smote.values\n",
    "y_train = y_train_smote.values\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "# For the test set, keep the original (imbalanced) distribution.\n",
    "X_test = X_test_ori.values\n",
    "y_test = y_test_ori.values\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Convert the datasets to PyTorch tensors.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097e86-3a59-4713-ae5e-9e13963d2250",
   "metadata": {},
   "source": [
    "# 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c920a6f-e5e7-4930-8333-433cb3c1b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Define the MLP Model\n",
    "# ---------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class FraudResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=3):\n",
    "        super(FraudResNet, self).__init__()\n",
    "        # Initial projection layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_dim) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8091ba9c-f82b-4753-910c-21a8562ea18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 2  # Two neurons for two classes\n",
    "num_blocks = 3  # Number of residual blocks\n",
    "\n",
    "model = FraudResNet(input_dim, hidden_dim, output_dim, num_blocks).to(device)\n",
    "# model = FraudResNetCNN(input_dim, num_channels=64, output_dim=2, num_blocks=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d903d9-6c7c-4c4c-bcb2-c3e7e8ac7762",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1 Undersampling + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fde5e0-15e9-4daf-a833-11687be13a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Setup Loss, Optimizer, and Training Parameters\n",
    "# ---------------------------\n",
    "# Since the training data is now balanced, we can use the default weights.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "best_loss = float('inf')\n",
    "best_model_path = \"./Model_Weight/ResNet_best_under_smote_BIS.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8169fa1-5ec4-46df-ac4e-a5b9c47d7cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.6858\n",
      "Epoch [10/100], Loss: 0.5663\n",
      "Epoch [15/100], Loss: 0.4993\n",
      "Epoch [20/100], Loss: 0.4563\n",
      "Epoch [25/100], Loss: 0.4214\n",
      "Epoch [30/100], Loss: 0.3944\n",
      "Epoch [35/100], Loss: 0.3750\n",
      "Epoch [40/100], Loss: 0.3618\n",
      "Epoch [45/100], Loss: 0.3521\n",
      "Epoch [50/100], Loss: 0.3442\n",
      "Epoch [55/100], Loss: 0.3380\n",
      "Epoch [60/100], Loss: 0.3329\n",
      "Epoch [65/100], Loss: 0.3287\n",
      "Epoch [70/100], Loss: 0.3252\n",
      "Epoch [75/100], Loss: 0.3221\n",
      "Epoch [80/100], Loss: 0.3193\n",
      "Epoch [85/100], Loss: 0.3169\n",
      "Epoch [90/100], Loss: 0.3147\n",
      "Epoch [95/100], Loss: 0.3127\n",
      "Epoch [100/100], Loss: 0.3109\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6. Training Loop with Checkpointing (Saving Best Model by Training Loss)\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save the model if training loss improved.\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938285cc-a4bd-4653-a7dd-5d235895b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on SetA (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92    400000\n",
      "           1       0.30      0.85      0.44     27655\n",
      "\n",
      "    accuracy                           0.86    427655\n",
      "   macro avg       0.64      0.86      0.68    427655\n",
      "weighted avg       0.94      0.86      0.89    427655\n",
      "\n",
      "[[343735  56265]\n",
      " [  4074  23581]]\n",
      "F1-score: 0.43871219802606487\n",
      "Precision-score: 0.29533101219848207\n",
      "Recall-score: 0.8526848671126379\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluation on the Test Set (with the Original Imbalanced Distribution)\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on SetA (Test Data):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4288d-5311-4efc-8efa-369c5e632a9f",
   "metadata": {},
   "source": [
    "### 4.2 Normal SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0de632-6b72-4a3e-907f-ceae43132d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Setup Loss, Optimizer, and Training Parameters\n",
    "# ---------------------------\n",
    "# Since the training data is now balanced, we can use the default weights.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "best_loss = float('inf')\n",
    "best_model_path = \"./Model_Weight/ResNet_best_smote_BIS.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d262433-b731-404c-bd32-85955ac7dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200], Loss: 0.7433\n",
      "Epoch [10/200], Loss: 0.5814\n",
      "Epoch [15/200], Loss: 0.5119\n",
      "Epoch [20/200], Loss: 0.4620\n",
      "Epoch [25/200], Loss: 0.4215\n",
      "Epoch [30/200], Loss: 0.3915\n",
      "Epoch [35/200], Loss: 0.3728\n",
      "Epoch [40/200], Loss: 0.3599\n",
      "Epoch [45/200], Loss: 0.3500\n",
      "Epoch [50/200], Loss: 0.3413\n",
      "Epoch [55/200], Loss: 0.3348\n",
      "Epoch [60/200], Loss: 0.3296\n",
      "Epoch [65/200], Loss: 0.3253\n",
      "Epoch [70/200], Loss: 0.3217\n",
      "Epoch [75/200], Loss: 0.3186\n",
      "Epoch [80/200], Loss: 0.3159\n",
      "Epoch [85/200], Loss: 0.3134\n",
      "Epoch [90/200], Loss: 0.3112\n",
      "Epoch [95/200], Loss: 0.3092\n",
      "Epoch [100/200], Loss: 0.3073\n",
      "Epoch [105/200], Loss: 0.3055\n",
      "Epoch [110/200], Loss: 0.3039\n",
      "Epoch [115/200], Loss: 0.3023\n",
      "Epoch [120/200], Loss: 0.3008\n",
      "Epoch [125/200], Loss: 0.2994\n",
      "Epoch [130/200], Loss: 0.2981\n",
      "Epoch [135/200], Loss: 0.2969\n",
      "Epoch [140/200], Loss: 0.2957\n",
      "Epoch [145/200], Loss: 0.2945\n",
      "Epoch [150/200], Loss: 0.2934\n",
      "Epoch [155/200], Loss: 0.2924\n",
      "Epoch [160/200], Loss: 0.2914\n",
      "Epoch [165/200], Loss: 0.2905\n",
      "Epoch [170/200], Loss: 0.2896\n",
      "Epoch [175/200], Loss: 0.2888\n",
      "Epoch [180/200], Loss: 0.2880\n",
      "Epoch [185/200], Loss: 0.2872\n",
      "Epoch [190/200], Loss: 0.2865\n",
      "Epoch [195/200], Loss: 0.2858\n",
      "Epoch [200/200], Loss: 0.2851\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6. Training Loop with Checkpointing (Saving Best Model by Training Loss)\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save the model if training loss improved.\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a914f799-1e17-40b6-9907-1dd2d5d7d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on SetA (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.92    200000\n",
      "           1       0.31      0.87      0.46     13828\n",
      "\n",
      "    accuracy                           0.87    213828\n",
      "   macro avg       0.65      0.87      0.69    213828\n",
      "weighted avg       0.95      0.87      0.89    213828\n",
      "\n",
      "[[173164  26836]\n",
      " [  1778  12050]]\n",
      "F1-score: 0.45718404977804755\n",
      "Precision-score: 0.3098801625263591\n",
      "Recall-score: 0.8714203066242406\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluation on the Test Set (with the Original Imbalanced Distribution)\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on SetA (Test Data):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5dbdf-2a55-4e5c-9dee-5296c2040a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95474316-e4c2-4438-9254-ae87abc3ad6b",
   "metadata": {},
   "source": [
    "# 5. Evaluate on Unseen Data (Without Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70969d-1ac3-4c99-9794-de6dac0ed3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./Model_Final/CNN_best_model_smote_final_BIS.pt\"\n",
    "\n",
    "print(best_model_path)\n",
    "loaded_model = FraudResNet(5, 128, 2, 5).to(device)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ced7a-233e-4e8d-b907-3e3e888270b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_files_unseen_1 = glob.glob(\"./BIS_data/client_part7_1.npy\")\n",
    "\n",
    "# Load each npy file into a list\n",
    "client_data_list_unseen_1 = [np.load(file, allow_pickle=True) for file in npy_files_unseen_1]\n",
    "\n",
    "# Combine all client data into one numpy array\n",
    "combined_data_unseen_1 = np.vstack(client_data_list_unseen_1)\n",
    "\n",
    "col_names = [f'pca_{i}' for i in range(5)] + ['laundering_schema_type', 'laundering_schema_id']\n",
    "\n",
    "# Convert the combined array into a DataFrame\n",
    "new_df = pd.DataFrame(combined_data_unseen_1, columns=col_names)\n",
    "new_df = new_df.drop('laundering_schema_id', axis=1)\n",
    "new_df = new_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "new_df['laundering_schema_type'] = new_df['laundering_schema_type'].notna().astype(int)\n",
    "new_df = new_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfda276-2d2b-423b-876d-0f1d1e982916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Fine-tuning\n",
    "# ---------------------------\n",
    "# 8. Load the Best Model into a New Instance and Test on Unseen Data (e.g., SetB)\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "# loaded_model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "if 'laundering_schema_type' in new_df.columns:\n",
    "    X_new = new_df.drop('laundering_schema_type', axis=1).values\n",
    "    y_new = new_df['laundering_schema_type'].values\n",
    "else:\n",
    "    X_new = new_df.values\n",
    "    y_new = None\n",
    "\n",
    "X_new = X_new.astype(np.float32)\n",
    "X_new = torch.tensor(X_new, dtype=torch.float).to(device)\n",
    "if y_new is not None:\n",
    "    y_new = torch.tensor(y_new, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_outputs = loaded_model(X_new)\n",
    "    _, new_predicted = torch.max(new_outputs, 1)\n",
    "    new_predictions = new_predicted.cpu().numpy()\n",
    "\n",
    "if y_new is not None:\n",
    "    # Convert y_new tensor to numpy array for metric calculations.\n",
    "    y_new_np = y_new.cpu().numpy()\n",
    "    print(\"\\nEvaluation on Unseen Data (Set7_part1):\")\n",
    "    print(classification_report(y_new_np, new_predictions))\n",
    "    print(confusion_matrix(y_new_np, new_predictions))\n",
    "    print(\"F1-score:\", f1_score(y_new_np, new_predictions))\n",
    "    print(\"Precision-score:\", precision_score(y_new_np, new_predictions))\n",
    "    print(\"Recall-score:\", recall_score(y_new_np, new_predictions))\n",
    "else:\n",
    "    print(\"\\nPredictions on Unseen Data (Set7_part1):\")\n",
    "    print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d720a-64a1-40ae-9326-7330f70b76e6",
   "metadata": {},
   "source": [
    "# 6. Evaluate on Unseen Data (With Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad42249-4baa-45d7-9747-6fa26ed161e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "best_model_path = \"./Model_Final/best_model_smote_final_all.pt\"\n",
    "\n",
    "# Separate features and labels.\n",
    "X_new = new_df.drop('Class', axis=1).values\n",
    "y_new = new_df['Class'].values\n",
    "\n",
    "# Split the unseen data into a fine-tuning training set and a test set.\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(\n",
    "    X_new, y_new, test_size=0.5, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors.\n",
    "X_new_train = torch.tensor(X_new_train, dtype=torch.float)\n",
    "y_new_train = torch.tensor(y_new_train, dtype=torch.long)\n",
    "X_new_test  = torch.tensor(X_new_test, dtype=torch.float)\n",
    "y_new_test  = torch.tensor(y_new_test, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load the Pre-Trained Model from SetA\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "class FraudMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FraudMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = X_new_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Two neurons for two classes\n",
    "\n",
    "model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "loaded_model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# It's often useful to set the model in train mode during fine-tuning.\n",
    "loaded_model.train()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Fine-Tuning on Unseen Data (SetB)\n",
    "# ---------------------------\n",
    "# Use a lower learning rate for fine-tuning.\n",
    "finetune_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-3)\n",
    "finetune_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "finetune_epochs = 300  # Adjust as needed\n",
    "\n",
    "print(\"\\n--- Fine-Tuning on Unseen Data (SetB) ---\")\n",
    "for epoch in range(finetune_epochs):\n",
    "    finetune_optimizer.zero_grad()\n",
    "    outputs = loaded_model(X_new_train)\n",
    "    loss = finetune_criterion(outputs, y_new_train)\n",
    "    loss.backward()\n",
    "    finetune_optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Fine-Tuning Epoch [{epoch + 1}/{finetune_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969268f-661b-48a7-8684-820ccc9aff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Evaluate the Fine-Tuned Model on SetB Test Data\n",
    "# ---------------------------\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = loaded_model(X_new_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    y_pred = predicted.numpy()\n",
    "    y_true = y_new_test.numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on Fine-Tuned Unseen Data (SetB Test):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5f4e2-3624-4167-9310-d3022cc554a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
