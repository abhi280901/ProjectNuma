{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adf668a-b68d-410e-a5a7-2a3df6994bc6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54964b5d-c5ad-4a78-9a55-95d3df31f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb820366-85c9-4d00-a079-52c298d393c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2138275 entries, 0 to 2138274\n",
      "Data columns (total 6 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   pca_0                   float64\n",
      " 1   pca_1                   float64\n",
      " 2   pca_2                   float64\n",
      " 3   pca_3                   float64\n",
      " 4   pca_4                   float64\n",
      " 5   laundering_schema_type  int64  \n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 97.9 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Shuffle the Dataset\n",
    "# ---------------------------\n",
    "df_full = pd.read_parquet('../BIS_data/Final_BIS_Data.parquet')\n",
    "df_full = df_full.reset_index(drop=True)\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031b6f1-15b1-4d22-ad45-72996ef14897",
   "metadata": {},
   "source": [
    "# 2. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b792d7-696a-4471-a6c8-71068d12bb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1489380 synthetic fraud samples using CTGAN.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Split Data into Train and Test Sets BEFORE Undersampling\n",
    "# ---------------------------\n",
    "X = df_full.drop('laundering_schema_type', axis=1)\n",
    "y = df_full['laundering_schema_type']\n",
    "\n",
    "# Use stratify=y to preserve the class distribution in the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "fraud_train = X_train[y_train == 1]\n",
    "nonfraud_train = X_train[y_train == 0]\n",
    "\n",
    "n_nonfraud = len(nonfraud_train)\n",
    "n_fraud = len(fraud_train)\n",
    "n_to_generate = n_nonfraud - n_fraud\n",
    "\n",
    "print(f\"Generating {n_to_generate} synthetic fraud samples using CTGAN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c16e92-da8f-4f6e-b71c-88e19bfcb58e",
   "metadata": {},
   "source": [
    "# 3. CTGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee82b0-f75a-4dc3-b531-285ac87e3a06",
   "metadata": {},
   "source": [
    "## 3.1 Default CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb22a67-9a55-4438-977e-6d2144bc026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-2.21) | Discrim. (0.00): 100%|████████████████████████████████████████████████| 300/300 [14:36<00:00,  2.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.648796</td>\n",
       "      <td>1.312458</td>\n",
       "      <td>-0.331564</td>\n",
       "      <td>-0.862923</td>\n",
       "      <td>0.907045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.041407</td>\n",
       "      <td>-0.874377</td>\n",
       "      <td>0.080091</td>\n",
       "      <td>0.436251</td>\n",
       "      <td>0.752192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.933969</td>\n",
       "      <td>-0.005520</td>\n",
       "      <td>-0.539325</td>\n",
       "      <td>-0.454675</td>\n",
       "      <td>0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.173809</td>\n",
       "      <td>1.779171</td>\n",
       "      <td>-0.485998</td>\n",
       "      <td>-0.406481</td>\n",
       "      <td>1.234283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.034956</td>\n",
       "      <td>0.105055</td>\n",
       "      <td>1.111455</td>\n",
       "      <td>-0.647845</td>\n",
       "      <td>1.430754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pca_0     pca_1     pca_2     pca_3     pca_4\n",
       "0  1.648796  1.312458 -0.331564 -0.862923  0.907045\n",
       "1  2.041407 -0.874377  0.080091  0.436251  0.752192\n",
       "2  0.933969 -0.005520 -0.539325 -0.454675  0.507787\n",
       "3  1.173809  1.779171 -0.485998 -0.406481  1.234283\n",
       "4  1.034956  0.105055  1.111455 -0.647845  1.430754"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. CTGAN Only the Training Set\n",
    "# ---------------------------\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(fraud_train)\n",
    "synthesizer = CTGANSynthesizer(metadata, cuda=True, verbose=True)\n",
    "synthesizer.fit(fraud_train)\n",
    "\n",
    "synthetic_data = synthesizer.sample(num_rows=n_to_generate)\n",
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b90369f-1e71-4533-843a-b42a5b493292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |██████████████████████████████████████████████████████| 5/5 [00:00<00:00, 104.23it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |███████████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.52it/s]|\n",
      "Column Shapes Score: 92.93%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.74it/s]|\n",
      "Column Pair Trends Score: 95.51%\n",
      "\n",
      "Overall Score (Average): 94.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "diagnostic = run_diagnostic(\n",
    "    real_data=fraud_train,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "quality_report = evaluate_quality(\n",
    "    fraud_train,\n",
    "    synthetic_data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bc5a0d-3095-4f96-9913-22ee3f520b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ctgan = pd.concat([nonfraud_train, fraud_train, synthetic_data], axis=0)\n",
    "y_train_ctgan = np.concatenate([np.zeros(n_nonfraud),np.ones(n_fraud), np.ones(n_to_generate)])\n",
    "\n",
    "Xy_train_ctgan = X_train_ctgan.copy()\n",
    "Xy_train_ctgan['laundering_schema_type'] = y_train_ctgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03dc1a11-6f58-468a-8bc2-869ece0811c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_ctgan = Xy_train_ctgan.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d18a09-1c4b-4c6b-922d-6b0ba9351851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xy_train_ctgan.to_csv(\"BIS_CTGAN_Full.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545aecb5-c1b9-4f5c-8319-44a37ed0ddd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4faf0b-8e0b-455f-a20d-e7d978456cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xy_train_ctgan = pd.read_csv(\"BIS_CTGAN_Full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e1a1b4-5fcc-403f-b8a4-47becaa46dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for the undersampled training data.\n",
    "X_train = Xy_train_ctgan.drop('laundering_schema_type', axis=1).values\n",
    "y_train = Xy_train_ctgan['laundering_schema_type'].values\n",
    "\n",
    "# For the test set, keep the original (imbalanced) distribution.\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Convert the datasets to PyTorch tensors.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96db7e-9139-4e0f-be42-c6c17efade24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Finetune CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76be91-e980-4107-8b24-759166f6a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(fraud_train)\n",
    "\n",
    "custom_synthesizer = CTGANSynthesizer(\n",
    "    metadata,\n",
    "    epochs=5000,\n",
    "    batch_size=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "custom_synthesizer.fit(fraud_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fd406-7e76-46b8-b6a0-0438141ffebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_customized = custom_synthesizer.sample(num_rows=n_to_generate)\n",
    "synthetic_data_customized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c858867-5892-4a88-9a63-beb7c32b6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "\n",
    "quality_report_custom = evaluate_quality(\n",
    "    fraud_train,\n",
    "    synthetic_data_customized,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c2823-ce00-40bd-bfb8-36e36c4726b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ctgan = pd.concat([nonfraud_train, fraud_train, synthetic_data_customized], axis=0)\n",
    "y_train_ctgan = np.concatenate([np.zeros(n_nonfraud),np.ones(n_fraud), np.ones(n_to_generate)])\n",
    "Xy_train_ctgan = X_train_ctgan.copy()\n",
    "Xy_train_ctgan['Class'] = y_train_ctgan\n",
    "Xy_train_ctgan = Xy_train_ctgan.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582c1d1-84aa-4141-9bbe-6b5909c0a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels for the undersampled training data.\n",
    "X_train = Xy_train_ctgan.drop('Class', axis=1).values\n",
    "y_train = Xy_train_ctgan['Class'].values\n",
    "\n",
    "# For the test set, keep the original (imbalanced) distribution.\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Convert the datasets to PyTorch tensors.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float).to(device)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0097e86-3a59-4713-ae5e-9e13963d2250",
   "metadata": {},
   "source": [
    "# 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c920a6f-e5e7-4930-8333-433cb3c1b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Define the MLP Model\n",
    "# ---------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class FraudResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=3):\n",
    "        super(FraudResNet, self).__init__()\n",
    "        # Initial projection layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_dim) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = 2  # Two neurons for two classes\n",
    "num_blocks = 3  # Number of residual blocks\n",
    "\n",
    "# model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "model = FraudResNet(input_dim, hidden_dim, output_dim, num_blocks).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35012a42-7e39-46f4-912d-3a47c52c620a",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fde5e0-15e9-4daf-a833-11687be13a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Setup Loss, Optimizer, and Training Parameters\n",
    "# ---------------------------\n",
    "# Since the training data is now balanced, we can use the default weights.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "best_loss = float('inf')\n",
    "best_model_path = \"./Model_Weight/ResNet_best_CTGAN_BIS.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8169fa1-5ec4-46df-ac4e-a5b9c47d7cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200], Loss: 0.6139\n",
      "Epoch [10/200], Loss: 0.5124\n",
      "Epoch [15/200], Loss: 0.4690\n",
      "Epoch [20/200], Loss: 0.4386\n",
      "Epoch [25/200], Loss: 0.4183\n",
      "Epoch [30/200], Loss: 0.4047\n",
      "Epoch [35/200], Loss: 0.3950\n",
      "Epoch [40/200], Loss: 0.3878\n",
      "Epoch [45/200], Loss: 0.3819\n",
      "Epoch [50/200], Loss: 0.3772\n",
      "Epoch [55/200], Loss: 0.3732\n",
      "Epoch [60/200], Loss: 0.3698\n",
      "Epoch [65/200], Loss: 0.3669\n",
      "Epoch [70/200], Loss: 0.3644\n",
      "Epoch [75/200], Loss: 0.3622\n",
      "Epoch [80/200], Loss: 0.3602\n",
      "Epoch [85/200], Loss: 0.3584\n",
      "Epoch [90/200], Loss: 0.3568\n",
      "Epoch [95/200], Loss: 0.3554\n",
      "Epoch [100/200], Loss: 0.3540\n",
      "Epoch [105/200], Loss: 0.3528\n",
      "Epoch [110/200], Loss: 0.3517\n",
      "Epoch [115/200], Loss: 0.3506\n",
      "Epoch [120/200], Loss: 0.3497\n",
      "Epoch [125/200], Loss: 0.3487\n",
      "Epoch [130/200], Loss: 0.3479\n",
      "Epoch [135/200], Loss: 0.3471\n",
      "Epoch [140/200], Loss: 0.3464\n",
      "Epoch [145/200], Loss: 0.3457\n",
      "Epoch [150/200], Loss: 0.3450\n",
      "Epoch [155/200], Loss: 0.3444\n",
      "Epoch [160/200], Loss: 0.3438\n",
      "Epoch [165/200], Loss: 0.3433\n",
      "Epoch [170/200], Loss: 0.3427\n",
      "Epoch [175/200], Loss: 0.3422\n",
      "Epoch [180/200], Loss: 0.3418\n",
      "Epoch [185/200], Loss: 0.3413\n",
      "Epoch [190/200], Loss: 0.3408\n",
      "Epoch [195/200], Loss: 0.3404\n",
      "Epoch [200/200], Loss: 0.3400\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6. Training Loop with Checkpointing (Saving Best Model by Training Loss)\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save the model if training loss improved.\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938285cc-a4bd-4653-a7dd-5d235895b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on SetA (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.87      0.92    400000\n",
      "           1       0.29      0.79      0.43     27655\n",
      "\n",
      "    accuracy                           0.86    427655\n",
      "   macro avg       0.64      0.83      0.67    427655\n",
      "weighted avg       0.94      0.86      0.89    427655\n",
      "\n",
      "[[346588  53412]\n",
      " [  5695  21960]]\n",
      "F1-score: 0.4262960194900366\n",
      "Precision-score: 0.2913548797962108\n",
      "Recall-score: 0.7940697884650154\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluation on the Test Set (with the Original Imbalanced Distribution)\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on SetA (Test Data):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86514af-bc1d-40b6-9253-2a58db83d0b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1 Fine Tune CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f835b-1431-4376-9aa3-e6e7ef026df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Setup Loss, Optimizer, and Training Parameters\n",
    "# ---------------------------\n",
    "# Since the training data is now balanced, we can use the default weights.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 200\n",
    "best_loss = float('inf')\n",
    "best_model_path = \"./Model_Weight/ResNet_best_CTGAN_FT_BIS.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a588b-34be-44ef-a85f-f69b029985f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6. Training Loop with Checkpointing (Saving Best Model by Training Loss)\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save the model if training loss improved.\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa89ba-ddfe-442c-9a68-e032106220f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7. Evaluation on the Test Set (with the Original Imbalanced Distribution)\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on SetA (Test Data):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95474316-e4c2-4438-9254-ae87abc3ad6b",
   "metadata": {},
   "source": [
    "# 5. Evaluate on Unseen Data (Without Fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0f56f-5185-4bc7-917f-f1a7d9d11bc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfda276-2d2b-423b-876d-0f1d1e982916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Fine-tuning\n",
    "# ---------------------------\n",
    "# 8. Load the Best Model into a New Instance and Test on Unseen Data (e.g., SetB)\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "print(best_model_path)\n",
    "loaded_model = FraudResNet(30, 128, 2, 5)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Assume you have an unseen dataset (SetB) with the same attributes.\n",
    "new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "\n",
    "if 'Class' in new_df.columns:\n",
    "    X_new = new_df.drop('Class', axis=1).values\n",
    "    y_new = new_df['Class'].values\n",
    "else:\n",
    "    X_new = new_df.values\n",
    "    y_new = None\n",
    "\n",
    "X_new = torch.tensor(X_new, dtype=torch.float)\n",
    "if y_new is not None:\n",
    "    y_new = torch.tensor(y_new, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_outputs = loaded_model(X_new)\n",
    "    _, new_predicted = torch.max(new_outputs, 1)\n",
    "    new_predictions = new_predicted.numpy()\n",
    "\n",
    "if y_new is not None:\n",
    "    # Convert y_new tensor to numpy array for metric calculations.\n",
    "    y_new_np = y_new.numpy()\n",
    "    print(\"\\nEvaluation on Unseen Data (SetB):\")\n",
    "    print(classification_report(y_new_np, new_predictions))\n",
    "    print(confusion_matrix(y_new_np, new_predictions))\n",
    "    print(\"F1-score:\", f1_score(y_new_np, new_predictions))\n",
    "    print(\"Precision-score:\", precision_score(y_new_np, new_predictions))\n",
    "    print(\"Recall-score:\", recall_score(y_new_np, new_predictions))\n",
    "else:\n",
    "    print(\"\\nPredictions on Unseen Data (SetB):\")\n",
    "    print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fe402-a642-4b5d-a828-bcb41706de76",
   "metadata": {},
   "source": [
    "### 5.1 CTGAN FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb55a3-4cb4-442d-9d44-b6efe290b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Fine-tuning\n",
    "# ---------------------------\n",
    "# 8. Load the Best Model into a New Instance and Test on Unseen Data (e.g., SetB)\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "print(best_model_path)\n",
    "loaded_model = FraudResNet(30, 128, 2, 5)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Assume you have an unseen dataset (SetB) with the same attributes.\n",
    "new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "\n",
    "if 'Class' in new_df.columns:\n",
    "    X_new = new_df.drop('Class', axis=1).values\n",
    "    y_new = new_df['Class'].values\n",
    "else:\n",
    "    X_new = new_df.values\n",
    "    y_new = None\n",
    "\n",
    "X_new = torch.tensor(X_new, dtype=torch.float)\n",
    "if y_new is not None:\n",
    "    y_new = torch.tensor(y_new, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_outputs = loaded_model(X_new)\n",
    "    _, new_predicted = torch.max(new_outputs, 1)\n",
    "    new_predictions = new_predicted.numpy()\n",
    "\n",
    "if y_new is not None:\n",
    "    # Convert y_new tensor to numpy array for metric calculations.\n",
    "    y_new_np = y_new.numpy()\n",
    "    print(\"\\nEvaluation on Unseen Data (SetB):\")\n",
    "    print(classification_report(y_new_np, new_predictions))\n",
    "    print(confusion_matrix(y_new_np, new_predictions))\n",
    "    print(\"F1-score:\", f1_score(y_new_np, new_predictions))\n",
    "    print(\"Precision-score:\", precision_score(y_new_np, new_predictions))\n",
    "    print(\"Recall-score:\", recall_score(y_new_np, new_predictions))\n",
    "else:\n",
    "    print(\"\\nPredictions on Unseen Data (SetB):\")\n",
    "    print(new_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d720a-64a1-40ae-9326-7330f70b76e6",
   "metadata": {},
   "source": [
    "# 6. Evaluate on Unseen Data (With Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad42249-4baa-45d7-9747-6fa26ed161e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"Fraud_dataset/Creditcard/GlobalUnseen.csv\")\n",
    "best_model_path = \"./Model_Final/best_model_ctgan_final_all.pt\"\n",
    "\n",
    "# Separate features and labels.\n",
    "X_new = new_df.drop('Class', axis=1).values\n",
    "y_new = new_df['Class'].values\n",
    "\n",
    "# Split the unseen data into a fine-tuning training set and a test set.\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(\n",
    "    X_new, y_new, test_size=0.5, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors.\n",
    "X_new_train = torch.tensor(X_new_train, dtype=torch.float)\n",
    "y_new_train = torch.tensor(y_new_train, dtype=torch.long)\n",
    "X_new_test  = torch.tensor(X_new_test, dtype=torch.float)\n",
    "y_new_test  = torch.tensor(y_new_test, dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load the Pre-Trained Model from SetA\n",
    "# ---------------------------\n",
    "# Create a new model instance with the same architecture.\n",
    "class FraudMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FraudMLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = X_new_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # Two neurons for two classes\n",
    "\n",
    "model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "loaded_model = FraudMLP(input_dim, hidden_dim, output_dim)\n",
    "loaded_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# It's often useful to set the model in train mode during fine-tuning.\n",
    "loaded_model.train()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Fine-Tuning on Unseen Data (SetB)\n",
    "# ---------------------------\n",
    "# Use a lower learning rate for fine-tuning.\n",
    "finetune_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-3)\n",
    "finetune_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "finetune_epochs = 300  # Adjust as needed\n",
    "\n",
    "print(\"\\n--- Fine-Tuning on Unseen Data (SetB) ---\")\n",
    "for epoch in range(finetune_epochs):\n",
    "    finetune_optimizer.zero_grad()\n",
    "    outputs = loaded_model(X_new_train)\n",
    "    loss = finetune_criterion(outputs, y_new_train)\n",
    "    loss.backward()\n",
    "    finetune_optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Fine-Tuning Epoch [{epoch + 1}/{finetune_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969268f-661b-48a7-8684-820ccc9aff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Evaluate the Fine-Tuned Model on SetB Test Data\n",
    "# ---------------------------\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = loaded_model(X_new_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    y_pred = predicted.numpy()\n",
    "    y_true = y_new_test.numpy()\n",
    "    \n",
    "    print(\"\\nEvaluation on Fine-Tuned Unseen Data (SetB Test):\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "    print(\"Precision-score:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall-score:\", recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5f4e2-3624-4167-9310-d3022cc554a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
